# 02. Terraform SSH

### Topics

- DigitalOcean Firewalls
- Terraform Modules
- SSH Key Setup in Terraform

### Introduction

In [01. Terraform Nodes](01.%20Terraform%20Nodes.md), we span up a node on DigitalOcean and then destroyed it utilising Terraform via docker-compose and a Makefile. 

We need to be able to SSH into the node. We should set up a firewall configuration using the [digitalocean_firewall](https://registry.terraform.io/providers/digitalocean/digitalocean/latest/docs/resources/firewall) resource in Terraform to do this, saying "we allow traffic through port 22", and then set up an SSH key.

We will use Cloudposse's [terraform-tls-ssh-key-pair module](https://github.com/cloudposse/terraform-tls-ssh-key-pair) to accomplish this very quickly, because it also generates the private key file you can then use to SSH in to a node. Normally you wouldn't do this and would use Terraform to handle all the things, but for now you can use this private key to SSH into the node.

When you SSH into a node using the private key created by the module above, the node IP will be added to your `known_hosts` file. When you destroy the nodes, we will want to remove this key from known-hosts if it exists, so we run a `local-exec` to remove the IP with `ssh-keygen -R`. You can see this under the `remove_known_hosts` null resource in [droplets.tf](../infra/terraform/droplets.tf). 

For now though, let's look at the SSH Key module.

### The SSH Key Module

Terraform modules are packages of configurations created by other users that you can use in your own configuration.

To use a module, once you've added the `module` key to your file, you have to run `terraform get` to import it. This has been added to the [Makefile](../Makefile) as a private target using the `--` convention (`--get`), so it doesn't need to be called directly - it is now automatically run whenever you `plan` or `provision`.

You'll notice that once `terraform get` has been run, the currently-git-ignored [.terraform](../infra/terraform/.terraform) directory will contain the new module configuration within a `modules` subdirectory.  

The SSH key module needs an SSH key directory as an argument so this can be configured via an environment variable, in keeping with how we have configured Terraform so far:

1. The environment variable has been added to [.env.template](../.env.template) and needs to be copied to [.env](../.env). 
2. This is then given the required `TF_VAR_` prefix and is passed to the terraform docker container in [docker-compose.yml](../docker-compose.yml).
3. The variable is added as an "input" in keeping with Terraform conventions within [inputs.tf](../infra/terraform/inputs.tf)
4. The variable can then be used in [droplets.tf](../infra/terraform/droplets.tf) for the module configuration.

There's more complexity in [droplets.tf](../infra/terraform/droplets.tf): we are now saying that "for each droplet, generate an SSH key file using our module, and use the contents of that key for the ssh key for this droplet". It's an extra step because of the module which also creates the physical key.

When you run `make provision` now, you'll see that an SSH key has been created in your SSH key directory (configurable, but by default: `~/.ssh/createinfra`). There's one final piece of the puzzle missing, though. You need to allow SSHing into the node over port 22 via a simple firewall configuration.

### Firewalls

Firewalls are a well known concept, and you can tell DigitalOcean to "allow traffic through this protocol (tcp / udp) and this port for this droplet" using the [digitalocean_firewall resource](https://registry.terraform.io/providers/digitalocean/digitalocean/latest/docs/resources/firewall).

This has been placed in [firewalls.tf](../infra/terraform/firewalls.tf) to maintain a clear separation of concerns - all firewall stuff can be placed here. In the future we may want to group this with a specific droplet, but for now this is good enough.

This configuration says:

- For all droplets, allow inbound traffic over TCP port 22 (the ssh port)
- For all droplets, allow any outbound traffic over TCP or UDP

When we add new containers in the future such as Kibana, we will need to look at the Kibana docs to find out which ports it uses and add firewall rules. Even docker swarm uses a range of ports to communicate within the swarm and these will need to be added as well for all nodes that want to be able to communicate with each other.

### Using this

- Make sure docker is running and you have `docker-compose` installed
- Copy `.env.template` to `.env` and fill in the configuration values (or add the new vars from 01. if you're following along)
- Run `make provision`, wait until it's finished, and go look at the DigitalOcean UI to see your new project containing a droplet
- Copy the IP from the DigitalOcean UI and use it to ssh into your server using: `ssh -i ~/.ssh/MyProject.mainnode root@YOUR_DROPLET_IP` (or whatever you named your project and droplet name).
- Check your `known_hosts` file with `cat ~/.ssh/known_hosts` and note that the IP of the node has been added here.
- Add a second node that you want spun up:
  - First add the name of your new node in [.env](../.env), such as `DIGITALOCEAN_DROPLET_NAME_2=droplet2`
  - Pass this to Terraform this via environment variable by prefixing it with `TF_VAR` in [docker-compose.yml](../docker-compose.yml), for example with: `TF_VAR_DIGITALOCEAN_DROPLET_NAME_2: ${DIGITALOCEAN_DROPLET_NAME_2`
  - Allow Terraform to use this as a variable by updating [inputs.tf](../infra/terraform/inputs.tf) and adding `DIGITALOCEAN_DROPLET_NAME_2` as a string, and with a description (in case you were running this without docker-compose / env vars)
  - Finally, update your droplets array in [droplets.tf](../infra/terraform/droplets.tf), with the key being `(var.DIGITALOCEAN_DROPLET_NAME_2)`
- Run `make provision` again, watch your new node being spun up. You will now have a second SSH key that only works for this node
- Run `make destroy`, wait until it's finished, and see how your project, droplets, ssh keys and any of their IPs added to `known_hosts` are cleaned up

### Next up

We need to install docker on our nodes so that we can have a docker swarm running on them and finally deploy software to them. For this, we will use docker-machine which handles the docker installation and setup for us automatically.