# 03. Docker Swarm

### Topics

- Installing docker and docker swarm
- Terraform provisioners (`remote-exec`, `local-exec`) (and downsides)
- Passing in objects as environment variables for nodes

### Introduction

Once we have spun up nodes with terraform, we need to do things on the nodes themselves, notably installing docker, setting up a swarm and other things.

We can use `remote-exec` and `local-exec` for these, but there are significant downsides which will be discussed later, so this step is a step we will take, then re-do differently in 04.

### Installing Docker on nodes

In the past, we could use docker-machine as a shortcut to install docker on nodes automatically for us. Unfortunately docker-machine was deprecated around 2021, and even though Rancher host an open source [fork](https://github.com/rancher/machine) of it for their own purposes, it isn't really a supported solution anymore. Instead, we need run a command to install docker on each node ourselves:

```bash
curl -fsSL https://get.docker.com -o get-docker.sh && sh get-docker.sh && rm get-docker.sh
```

This is added under `remote-exec` in [droplets.tf](../infra/terraform/droplets.tf), along with a `connection` key to specify how to connect over SSH. With this, all nodes will have docker installed. But we still need to tell docker swarm which node is a manager, which is a worker, and to tell the worker to join the manager's swarm.

### Docker Swarm

Docker swarm requires a minimum of three nodes to build a swarm: one manager node and two worker nodes. For this we need to update [firewalls.tf](../infra/terraform/firewalls.tf) with the ports that docker swarm needs to communicate between nodes.

We have placed the swarm setup into [docker.tf](../infra/terraform/docker.tf) as it will be removed at a later date, but for now you can see that the order of setting up a swarm is:

- SSH into the manager node with `-q -o StrictHostKeyChecking=no -o CheckHostIP=no -o UserKnownHostsFile=/dev/null` so that we can ignore the prompt to add the key to our `known_hosts` file (we need to skip this for the automation to work)
- Pass the correct key for the node with `-i`
- Call `docker-swarm init` and grep for the command returned that we need to run on the worker nodes
- Use the result of stdout for the command we run on the worker nodes

It's not trivial to get the result of a `local-exec`, so we have to use a [module](https://registry.terraform.io/modules/Invicton-Labs/shell-data/external/latest) that wraps running shell scripts as "data sources" so that we can capture the output.

The reason Terraform doesn't support this easily by default is because provisioners (`local-exec`, `remote-exec`) extend into the space of configuration management (what we are doing here) software, but with low confidence. You can read more about this [here](https://spacelift.io/blog/terraform-provisioners), but the short of it is:

> If, for some reason, the provisioner tasks fail to run on a few machines, it just increases the overhead of identifying them and deploying a workaround. Gaining an understanding of why a particular provisioner did not work on a set of machines can be very difficult and is highly hostile. Simply because there are several factors that are potentially unique to each resource.

You will experience this, for example: a network hiccup on DigitalOcean's side has happened even while I've been running `make provision` to test things whilst writing this, and I couldn't figure out which nodes had had docker installed on them and which had not. Ansible can help us here.

Another way to set up docker swarm with Terraform would be to write bash scripts, upload these to the servers and run them, but this would take us even longer to write and set up.

Ansible is a very useful tool for such tasks because it can detect what is successful or not by looking at the output of another command before deciding to run. For example, if we told ansible to install docker, ansible could run the `docker` command, parse the output (stdout) and if it says something like "docker: file not found in..." then Ansible would know that docker has not yet been installed, and then it would actually install it. Subsequent runs would see that there is another output from running `docker`, so it would know it's installed and would skip trying to install it. 

This is all easily configurable with ansible. For now, we have set up docker swarm with Terraform providers. Next we will switch to Ansible for our docker and docker swarm setup.

### Nodes in `.env`

One of the annoying things we had in 02. was having to changes things in multiple places if we wanted to add new nodes. We would have had to alter:

- `.env`
- `docker-compose.yml`
- `inputs.tf`
- `The file which had locals in it before`

Now though, [.env](../.env) has been updated to take in an object into the environment variable `NODES`, this is passed into as `TF_VAR_NODES` in [docker-compose.yml](../docker-compose.yml), and there is an object map in [inputs.tf](../infra/terraform/inputs.tf) which takes this environment variable and parses it into a map, all automatically.

This means that all we need to do to add a new node now is update the `NODES` environment variable in [.env](../.env) and call `terraform apply`. Awesome!

### Using this

- Make sure docker is running and you have `docker-compose` installed
- Copy `.env.template` to `.env` and fill in the configuration values (or add the new vars from 02. if you're following along, and delete the old ones)
- Run `make provision`, wait until it's finished, and go look at the DigitalOcean UI to see your new project containing the nodes you configured
- Add another node that you want spun up:
    - Add the new configuration in [.env](../.env) within `NODES` (make sure it has the `worker` tag, not `manager`)
- Run `make provision` again, watch your new node being spun up. You will now have another SSH key that only works for this node
- SSH into the main manager node and run `docker node ls`, which will list all the nodes that the docker swarm manager knows about - note that all nodes are in a swarm with one manager
- Don't forget to run `make destroy` to get rid of your nodes afterward

### Next up

We will switch to a configuration management tool like Ansible that can _reliably_ handle the installation of docker and docker-swarm instead of using provisioners in Terraform. In doing so, we relegate Terraform a tool that we use only for the provisioning of the nodes themselves and greatly simplify our setup (no need for [docker.tf](../infra/terraform/docker.tf) or other module hacks anymore).